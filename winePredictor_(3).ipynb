{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sumitk99/122ec0011/blob/master/winePredictor_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uUEzkU8O9Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa7560b-4c61-4aa3-e7e1-119f16b2a345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.0/433.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.1/755.1 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.31.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 6.31.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade weaviate-client rapidfuzz openai tqdm pandas -q\n",
        "\n",
        "!pip install -q transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SRbJk9aXWUuN",
        "outputId": "99070c7e-8545-4dc5-b6cf-cee86735b03f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Connected to Weaviate\n",
            "📥 Upload your CSV …\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-964902c6-440f-475c-83e8-757018a6f5b3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-964902c6-440f-475c-83e8-757018a6f5b3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving wine-list (1).xlsx to wine-list (1) (13).xlsx\n",
            "📝 Prepared 48 rows\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔄 Uploading: 100%|██████████| 1/1 [00:13<00:00, 13.93s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All data pushed to Weaviate\n",
            "\n",
            "🔍 Ask anything about wines. Type 'exit' to quit.\n",
            "\n",
            "🔍 Corrected Query: List Bordeaux wines from the 2010s\n",
            "🍷 Answer: Here are the Bordeaux wines from the 2010s:\n",
            "\n",
            "1. Wine Name: Chateau Petit-Village Pomerol 2010 (12x75cl)\n",
            "   - Original Name: Petit Village  2010\n",
            "   - Vintage: 2010\n",
            "   - Case Size: 12x75cl\n",
            "   - Country: France\n",
            "   - Region: Bordeaux\n",
            "   - Sub-Region: Pomerol\n",
            "   - Producer: Chateau Petit-Village\n",
            "\n",
            "2. Wine Name: Chateau Haut-Bailly Cru Classe Pessac-Leognan 2010 (12x75cl)\n",
            "   - Original Name: Haut Bailly Cru Classe Graves 2010\n",
            "   - Vintage: 2010\n",
            "   - Case Size: 12x75cl\n",
            "   - Country: France\n",
            "   - Region: Bordeaux\n",
            "   - Sub-Region: Pessac-Leognan\n",
            "   - Producer: Chateau Haut-Bailly \n",
            "\n",
            "🔍 Corrected Query: List Bordeaux **wines** from the 2010s to 2015\n",
            "🍷 Answer: Given the context provided, here are the Bordeaux wines from the 2010s:\n",
            "\n",
            "1. Chateau Petit-Village Pomerol 2010 by Chateau Petit-Village. This is a Pomerol wine from the Bordeaux region of France.\n",
            "\n",
            "2. Chateau Haut-Bailly Cru Classe Pessac-Leognan 2010 by Chateau Haut-Bailly. This is a Pessac-Leognan wine from the Bordeaux region of France.\n",
            "\n",
            "Please note that unless there are additional instances of Bordeaux wines from 2010 to 2015 not included in the context, these are the available options. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-1433528301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🔍 Ask anything about wines. Type 'exit' to quit.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0muser_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🍷 User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"👋 Exiting. Enjoy your wine!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import weaviate, pandas as pd, time, re\n",
        "from rapidfuzz import process\n",
        "from google.colab import files, userdata\n",
        "from weaviate.auth import AuthApiKey\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "WEAVIATE_API_KEY = userdata.get(\"Weaviate\").strip()\n",
        "WEAVIATE_URL     = \"https://5yinmh2nt6oomas2p5uxaw.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "OPENAI_API_KEY   = userdata.get(\"OpenAI\")\n",
        "\n",
        "\n",
        "client_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url      = WEAVIATE_URL,\n",
        "    auth_credentials = AuthApiKey(WEAVIATE_API_KEY),\n",
        "    headers          = {\"X-OpenAI-Api-Key\": OPENAI_API_KEY},\n",
        ")\n",
        "collection = client.collections.get(\"LWIN\")\n",
        "print(\"✅ Connected to Weaviate\")\n",
        "\n",
        "print(\"📥 Upload your CSV …\")\n",
        "uploaded = files.upload()\n",
        "csv_path = next(iter(uploaded))\n",
        "\n",
        "df = pd.read_excel(csv_path)\n",
        "\n",
        "def to_graphql_name(col):\n",
        "    col = col.strip()\n",
        "    col = re.sub(r'[\\s\\-]+', '_', col)\n",
        "    col = re.sub(r'[^0-9a-zA-Z_]', '', col)\n",
        "    if not re.match(r'^[_A-Za-z]', col):\n",
        "        col = '_' + col\n",
        "    return col\n",
        "\n",
        "df.columns = [to_graphql_name(c) for c in df.columns]\n",
        "def infer_column_type(series):\n",
        "    try:\n",
        "        pd.to_numeric(series.dropna())\n",
        "        return \"number\"\n",
        "    except Exception:\n",
        "        return \"string\"\n",
        "\n",
        "column_types = {col: infer_column_type(df[col]) for col in df.columns}\n",
        "for col in df.columns:\n",
        "    if column_types[col] == \"number\":\n",
        "        df[col] = df[col].where(df[col].notnull(), 0)\n",
        "    else:\n",
        "        df[col] = df[col].fillna('')\n",
        "wine_data = []\n",
        "texts = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    obj = {}\n",
        "    for col, value in row.items():\n",
        "        if column_types[col] == \"number\":\n",
        "            try:\n",
        "                obj[col] = float(value) if value != '' else None\n",
        "            except Exception:\n",
        "                obj[col] = None\n",
        "        else:\n",
        "            obj[col] = str(value)\n",
        "    obj['text'] = \" | \".join(f\"{k}: {v}\" for k, v in obj.items())\n",
        "    wine_data.append(obj)\n",
        "    texts.append(obj['text'])\n",
        "\n",
        "print(f\"📝 Prepared {len(wine_data)} rows\")\n",
        "\n",
        "BATCH = 500\n",
        "for start in tqdm(range(0, len(wine_data), BATCH), desc=\"🔄 Uploading\"):\n",
        "    batch_objs = wine_data[start:start+BATCH]\n",
        "    batch_txts = texts[start:start+BATCH]\n",
        "\n",
        "    resp    = client_openai.embeddings.create(\n",
        "        input=batch_txts, model=\"text-embedding-3-small\"\n",
        "    )\n",
        "    vectors = [d.embedding for d in resp.data]\n",
        "\n",
        "    for obj, vec in zip(batch_objs, vectors):\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                collection.data.insert(properties=obj, vector=vec)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt == 2:\n",
        "                    print(f\"❌ skipped {obj.get('product_name', obj.get('wine-name', 'Unknown'))} → {e}\")\n",
        "                else:\n",
        "                    time.sleep(1)\n",
        "print(\"✅ All data pushed to Weaviate\")\n",
        "\n",
        "wine_name_col = None\n",
        "for col in df.columns:\n",
        "    if 'name' in col.lower():\n",
        "        wine_name_col = col\n",
        "        break\n",
        "if wine_name_col is None:\n",
        "    raise ValueError(\"No column found for wine names!\")\n",
        "\n",
        "wine_names = df[wine_name_col].dropna().unique().tolist()\n",
        "\n",
        "def fuzzy_correct(token: str) -> str | None:\n",
        "    match = process.extractOne(token, wine_names, score_cutoff=85)\n",
        "    return match[0] if match else None\n",
        "\n",
        "def gpt_correct(sentence: str) -> str:\n",
        "    resp = client_openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a spell corrector for wine names. \"\n",
        "                        \"Only fix the wine term; keep the rest unchanged.\"},\n",
        "            {\"role\": \"user\", \"content\": sentence}\n",
        "        ]\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def smart_correct(user_q: str) -> str:\n",
        "    corrected = user_q\n",
        "    changed   = False\n",
        "    for token in re.findall(r\"[A-Za-z\\-']+\", user_q):\n",
        "        c = fuzzy_correct(token)\n",
        "        if c and c.lower() != token.lower():\n",
        "            corrected = re.sub(rf\"\\b{re.escape(token)}\\b\", c, corrected, flags=re.IGNORECASE)\n",
        "            changed   = True\n",
        "    return corrected if changed else gpt_correct(user_q)\n",
        "\n",
        "def embed(q: str):\n",
        "    return client_openai.embeddings.create(\n",
        "        input=[q], model=\"text-embedding-3-small\"\n",
        "    ).data[0].embedding\n",
        "\n",
        "# ── 🔍 INTERACTIVE LOOP ────────────────────────────────────────────\n",
        "print(\"\\n🔍 Ask anything about wines. Type 'exit' to quit.\\n\")\n",
        "while True:\n",
        "    user_q = input(\"🍷 User: \").strip()\n",
        "    if user_q.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"👋 Exiting. Enjoy your wine!\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        corrected_q = smart_correct(user_q)\n",
        "        print(\"🔍 Corrected Query:\", corrected_q)\n",
        "\n",
        "        q_vec = embed(corrected_q)\n",
        "\n",
        "        res = collection.query.hybrid(\n",
        "            query=corrected_q,\n",
        "            vector=q_vec,\n",
        "            alpha=0.6,\n",
        "            limit=10\n",
        "        )\n",
        "\n",
        "        context = \"\\n\".join(o.properties[\"text\"] for o in res.objects)\n",
        "        if not context:\n",
        "            print(\"⚠️  No context found, trying exact LWIN fallback…\")\n",
        "        if not context:\n",
        "            id_match = re.search(r\"\\d{7}\", corrected_q)\n",
        "            if id_match:\n",
        "                lwin_id = id_match.group(0)\n",
        "                res = collection.query.with_where({\n",
        "                    \"path\": [\"lwin7\"],\n",
        "                    \"operator\": \"Equal\",\n",
        "                    \"valueText\": lwin_id\n",
        "                }).limit(1)\n",
        "                context = \"\\n\".join(o.properties[\"text\"] for o in res.objects)\n",
        "\n",
        "        if not context:\n",
        "            context = \"No relevant context found.\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a wine expert. Answer precisely using the context. Do all the filtering according to user's query.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Context:\\n{context}\\n\\nQuestion: {corrected_q}\"}\n",
        "        ]\n",
        "\n",
        "        answer = client_openai.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages\n",
        "        ).choices[0].message.content.strip()\n",
        "\n",
        "        print(\"🍷 Answer:\", answer, \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error:\", e, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZHhbnQE9bsb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}